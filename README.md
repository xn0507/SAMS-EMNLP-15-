# Generic Deep Compositional Distributional Models

## Introduction
In this project, we are interested in obtaining generic deep compositional distributional models that are not tired in any sepecific task.
This is helpful for the task where there does not exsit enough training data for a deep model. It is suggested to train a general sentence composioner with liguistic insights first, after which the model parameters can be further tuned in a task-specific dataset.

## Methodology
We use recursive/recurrent neural network here to score sentence plausibility. This can be viewed as a sentence-level extension of the Collobert and Weston Embeddings.

## Usage
Under development.

## Contact
For questions please contact cheng6076@gmail.com.
